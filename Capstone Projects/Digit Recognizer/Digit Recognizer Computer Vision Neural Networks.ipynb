{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils.np_utils import to_categorical #convert to one-hot encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109a819d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_train = train['label']\n",
    "X_train = train.drop(labels=[\"label\"], axis=1)\n",
    "g=sns.countplot(Y_train)\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, a network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks, most commonly applied to analyzing visual imagery.\n",
    "\n",
    "CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.[1] They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. To converge Convolution Neural Networks faster grayscale is better than 0-255 color pallete illumination. Image reconition is a complex process for a computer since it sees is this matrix, or this grid, of pixel intensity values that tells us the brightness of each pixel in the image. So the computer vision problem is to look at this matrix of pixel intensity values, and tell us that these numbers represent the door handle of a car. Concretely, when we use machine learning to build a car detector, what we do is we come up with a label training set, with, let's say, a few label examples of cars and a few label examples of things that are not cars, then we give our training set to the learning algorithm trained a classifier and then, you know, we may test it and show the new image and ask, \"What is this new thing?\". And hopefully it will recognize that that is a car. What is the dimension of the feature space? Suppose we were to use just 50 by 50 pixel images. Now that suppose our images were pretty small ones, just 50 pixels on the side. Then we would have 2500 pixels, \n",
    "and so the dimension of our feature size will be N equals 2500 where our feature vector x is a list of all the pixel testings, you know, the pixel brightness of pixel one, the brightness of pixel two, and so on down to the pixel brightness of the last pixel where, you know, in a typical computer representation, each of these may be values between say 0 to 255 if it gives us the grayscale value. So we have n equals 2500, and that's if we were using grayscale images. If we were using RGB images with separate red, green and blue values, we would have n equals 7500.\n",
    "So, if we were to try to learn a nonlinear hypothesis by including all the quadratic features, that is all the terms of the form, you know, Xi times Xj, while with the 2500 pixels we would end up with a total of three million features. And that's just too large to be reasonable; the computation would be very expensive to find and to represent all of these three million features per training example. \n",
    "8:55\n",
    "So, simple logistic regression together with adding in maybe the quadratic or the cubic features - that's just not a good way to learn complex nonlinear hypotheses when n is large because you just end up with too many features. In the next I would like to tell you about Neural Networks, which turns out to be a much better way to learn complex hypotheses, complex nonlinear hypotheses even when your input feature space, even when n is large. And along the way I'll also get to show you a of historically important applications \n",
    "of Neural networks\n",
    "Neural Networks are a pretty old algorithm that was originally motivated by the goal of having machines that can mimic the brain. \n",
    "Neuroscientists have done the following fascinating experiments where you cut the wire from the ears to the auditory cortex and you re-wire, \n",
    "in this case an animal's brain, so that the signal from the eyes to the optic nerve eventually gets routed to the auditory cortex. \n",
    "If you do this it turns out, the auditory cortex will learn \n",
    "to see. And this is in every single sense of the word see as we know it. So, if you do this to the animals, the animals can perform visual discrimination task and as they can look at images and make appropriate decisions based on the images and they're doing it with that piece of brain tissue.  Because of this and other similar experiments, these are called neuro-rewiring experiments. \n",
    "There's this sense that if the same piece of physical brain tissue can process sight or sound or touch then maybe there is one learning algorithm that can process sight or sound or touch. And instead of needing to implement a thousand different programs or a thousand different algorithms to do, you know, the thousand wonderful things that the brain does, maybe what we need to do is figure out some approximation or to whatever the brain's learning algorithm is and implement that and that the brain learned by itself how to process these different types of data. \n",
    "To a surprisingly large extent, it seems as if we can plug in almost any sensor to almost any part of the brain and so, within the reason, the brain will learn to deal with it. \n",
    "how a Neural Network can be used to compute the functions x1 AND x2, and the function x1 OR x2 when x1 and x2 are binary, that is when they take on values 0,1. We can also have a network to compute negation, that is to compute the function not x1.\n",
    " Cells include negations, the general idea is to put that large negative weight in front of the variable you want to negate. Minus 20 multiplied by x1 and that's the general idea of how you end up negating x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing to gray scale\n",
    "X_train = X_train/255.0\n",
    "test = test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=2\n",
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108104750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = plt.imshow(X_train[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "#                  activation ='relu', input_shape = (28,28,1)))\n",
    "# model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "# model.add(Dense(256, activation = \"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the optimizer\n",
    "# optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "# Compile the model\n",
    "model.compile(optimizer = \"adam\" , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set a learning rate annealer\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "#                                             patience=3, \n",
    "#                                             verbose=1, \n",
    "#                                             factor=0.5, \n",
    "#                                             min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "# batch_size = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         zoom_range = 0.1, # Randomly zoom image \n",
    "#         width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "#         height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "#         horizontal_flip=False,  # randomly flip images\n",
    "#         vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "# datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "#                               epochs = epochs, validation_data = (X_val,Y_val),\n",
    "#                               verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "#                               , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convolutional Neural Network (CNN)\n",
    "\n",
    "# # step 1 - Initialising the CNN\n",
    "# classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 2 - Convolution\n",
    "# classifier.add(Conv2D(32, (3, 3), padding = 'Same', activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "\n",
    "# # step 3-  Pooling\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# # adding another convulationary layer to make it deep neural net model \n",
    "# classifier.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 4 - Flattening\n",
    "# classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 5- Full connection (hidden layers)\n",
    "# classifier.add(Dense(output_dim = 256, activation = 'relu'))\n",
    "\n",
    "# # adding output layer\n",
    "# classifier.add(Dense(output_dim = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-6 compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step-7 fitting the CNN\n",
    "# epochs=30\n",
    "# batch_size=90\n",
    "\n",
    "# classifier.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "37800/37800 [==============================] - 4s 102us/step - loss: 0.2703 - acc: 0.9193\n",
      "Epoch 2/3\n",
      "37800/37800 [==============================] - 3s 83us/step - loss: 0.1101 - acc: 0.9658\n",
      "Epoch 3/3\n",
      "37800/37800 [==============================] - 3s 90us/step - loss: 0.0758 - acc: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a1d2f9f90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200/4200 [==============================] - 0s 50us/step\n",
      "0.10066649453448398\n",
      "0.9678571428571429\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(X_val, Y_val)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_model() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9b7d308dced3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Finally, with your model, you can save it super easily:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#from tensorflow.python.keras.models import save_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epic_num_reader.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save_model() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "#Finally, with your model, you can save it super easily:\n",
    "#from tensorflow.python.keras.models import save_model\n",
    "tf.keras.models.save_model('epic_num_reader.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load it back:\n",
    "\n",
    "new_model = tf.keras.models.load_model('epic_num_reader.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8be221d46b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#finally, make predictions!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "#finally, make predictions!\n",
    "\n",
    "predictions = new_model.predict(test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
